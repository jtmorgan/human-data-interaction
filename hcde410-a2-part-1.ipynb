{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Exploring sources of bias in data\n",
    "## Bias in data, demystified\n",
    "\n",
    "**All datasets have limitations. Any of these limitations can be a potential source of bias.** In the context of data, a \"bias\" is really just a divergence between what a dataset is supposed to capture about the world--according to the *dataset designer's intention*, or according to the *end-user's expectations*--and what's actually represented in the data. \n",
    "\n",
    "So biases in data are often highly *contextual*, which can make them subtle and hard to spot. Similarly, it's hard to predict ahead of time what the *consequences* of those biases might be, because it depends on what the data is being used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless some sources of \"bias\" in data show up over and over again. Some of these are:\n",
    "- **duplicated data:** the same data shows up multiple times\n",
    "- **incomplete data:** some data is missing from the dataset\n",
    "- **misleading data:** it looks like a piece of data means one thing, but it actually means something different\n",
    "- **unrepresentative data:** the dataset doesn't represent the population it's gathered from OR the population it's intended to model\n",
    "\n",
    "\n",
    "Any of these sources of bias, unless their properly documented (with a data statement, etc) can have unintended consequences: they cause a researcher who is using the data to reach incorrect conclusions, or cause a machine learning model that is trained on that dataset to make classification errors.\n",
    "\n",
    "The nature or scale of these consequences can be hard to predict. That's why any time you prepare to use data that you didn't gather yourself, it pays to spend some time exploring the dataset, identifying limitations, and thinking critically about how these limitations might affect your analysis or your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to assignment 2\n",
    "For this assignment, you'll be working with the Wikipedia Talk Corpus. You have already created a data statement for this dataset in class, so you're an 'expert' on it compared to most people! For more background on the Wikipedia Talk Corpus, please review the background information doc. LINKME\n",
    "\n",
    "In assignment 1, you learned how to process and analyze a dataset created by someone else. In that assignment, we took the dataset at face value: we assumed that there were no major errors, no missing data, nothing that would skew our results. We assumed that the dataset really did accurately capture all the bike and pedestrian traffic on the Burke-Gilman trail over the specified period of time.\n",
    "\n",
    "In assignment 2, we'll also be analyzing a dataset created by someone else, but this time *we won't assume that the data is complete and correct.* Instead, we will try to identify ways in which the data might be WRONG, and form hypotheses about how the limitations we discover might make it an unsuitable, or at least a potentially problematic, training dataset for a general-purpose hostile speech detector. \n",
    "\n",
    "In part 1 of the assignment, you will load one type of Wikipedia Talk Corpus data--the demographics of the crowdworkers who labelled the comments--into your copy of this Jupyter Notebook, and we will walk through a series of data processing steps, with the goal of ending up with a complete and accurate set of data about all of those crowdworkers. In the process, we will discover and discuss several limitations of the dataset.\n",
    "\n",
    "In part 2 of the assignment, you will generate some basic descriptive statistics about the demographics of the crowdworkers described in that dataset. If you are comfortable in Python, you can perform that analysis in this Notebook. If you aren't as comfortable in Python (yet!), you can perform the analysis in Google Sheets (just be sure to link to that Google Sheet from this notebook, and set the permissions so that your instructors can view it!). \n",
    "\n",
    "In part 3 of the assignment, you will answer some additional research questions about this dataset. Some of these questions can be answered without writing additional code or analyzing additional data; other questions will require you to combine this dataset another dataset in the corpus. You will have the option to choose whether you want to answer code or no-code questions.\n",
    "\n",
    "Whether you choose code or no-code question for part 3, you will need to write your responses to the question within this notebook, and submit a link to this notebook for grading.\n",
    "\n",
    "Since the purpose of this class is to get you comfortable thinking critically (like a researcher), rather than programming perfectly, you won't be graded on your code. You'll be graded on how well you reflect on the implications of your findings. If you still aren't sure what that means by the time you get to Part 3, ask your instructor or TA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cleaning and analyzing the annotator demographic data\n",
    "\n",
    "For the first part of this assignment, we're going to prepare one set of Wikipedia Talk data--the annotator demographics files--for analysis. In the process we'll perform a few \"sanity checks\" to make sure we understand what the data means, and know any limitations.\n",
    "\n",
    "This sort of \"[data wrangling](https://en.wikipedia.org/wiki/Data_wrangling)\" is a critical, if sometimes tedious, first step for any quantitative research project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the data into the notebook\n",
    "\n",
    "According to the documetation, the worker demographic data for the Wikipedia Talk Corpus is spread across three files:\n",
    "- ``toxicity_worker_demographics.tsv``\n",
    "- ``aggression_worker_demographics.tsv``\n",
    "- ``attack_worker_demographics.tsv``\n",
    "\n",
    "We will need to combine the data in these three files to come up with our canonical list of workers.\n",
    "\n",
    "First we'll load each of the annotator datafiles into our Notebook and save them into data structures that's easy to work with. In this case, I'm choosing to save each of these files as a list-of-dictionaries, since that's fairly standard, and it makes it easy to check your work as you go.\n",
    "\n",
    "By the way: ``.tsv`` stands for \"tab-separated values\", and it means that this file is organized into rows and columns, like a spreadsheet, and the data values for each column are separated by \"tab\" characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the csv module, a little code toolkit for working with spreadsheet-style data files\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will load in a tab-separated (.tsv) file and convert it into a lists-of-dictionaries. \n",
    "\n",
    "If you don't have much experience with Python, this (and some of the other code in this notebook) might be hard to understand. That's okay! For now, it's most important that you know what it does.\n",
    "\n",
    "If you have a ***lot*** of experience with Python, the code in this notebook might seem really, really primitive. That's also okay! Remember: in this course we're primarily interested in data, not code. Code is just one of the many tools we use to ask and answer questions about data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(file_path):\n",
    "    \"\"\" \n",
    "    Accepts: path to a tab-separated plaintext file\n",
    "    Returns: a list containing a dictionary for every row in the file, \n",
    "        with the file column headers as keys\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(file_path) as infile:\n",
    "        reader = csv.DictReader(infile, delimiter='\\t')\n",
    "        list_of_dicts = [dict(r) for r in reader]\n",
    "        \n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Identifying duplicated datafiles\n",
    "Let's load our three .tsv files into Python and store them as three variables with relevant names, so that we know which is which. \n",
    "\n",
    "Once we've created these three lists-of-dicts, we will do two things to check our work so far: \n",
    "- we will print the first annotator's demographic data (list index ``[0]``) so that we know what the format looks like\n",
    "- we will print the length of each list (the ``len`` function), to see how many rows is in each file. Each row should correspond to one crowdworker/annotator.\n",
    "\n",
    "***Note:*** for the cell below to run, your version of these datafiles and folders will need have the same names as the ones below, and your version of this Notebook will need to be stored in the same directory as the three folders that hold the datafiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'worker_id': '85', 'gender': 'female', 'english_first_language': '0', 'age_group': '18-30', 'education': 'bachelors'}\n",
      "3591\n",
      "{'worker_id': '833', 'gender': 'female', 'english_first_language': '0', 'age_group': '45-60', 'education': 'bachelors'}\n",
      "2190\n",
      "{'worker_id': '833', 'gender': 'female', 'english_first_language': '0', 'age_group': '45-60', 'education': 'bachelors'}\n",
      "2190\n"
     ]
    }
   ],
   "source": [
    "#load the data from the flat files into three lists-of-dictionaries\n",
    "toxicity_annotators = prepare_datasets(\"Wikipedia_Talk_Labels_Toxicity_4563973/toxicity_worker_demographics.tsv\")\n",
    "print(toxicity_annotators[0])\n",
    "print(len(toxicity_annotators))\n",
    "\n",
    "attack_annotators = prepare_datasets(\"Wikipedia_Talk_Labels_Personal_Attacks_4054689/attack_worker_demographics.tsv\")\n",
    "print(attack_annotators[0])\n",
    "print(len(attack_annotators))\n",
    "\n",
    "aggression_annotators = prepare_datasets(\"Wikipedia_Talk_Labels_Personal_Attacks_4054689/attack_worker_demographics.tsv\")\n",
    "print(aggression_annotators[0])\n",
    "print(len(aggression_annotators))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! This tells us a few things that we didn't know before:\n",
    "1. it looks like the demographic data matches what's listed in [the schema](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release#Schema_for_{attack/aggression/toxicity}_worker_demographics.tsv), which is great!\n",
    "2. it looks like the \"toxicity\" dataset was annotated by a lot more people (3,591) than the \"attack\" or \"aggression\" datasets (2,190)\n",
    "3. ``aggression_worker_demographics.tsv`` and ``attack_worker_demographics.tsv`` seem to contain the same number of workers, and the worker at the beginning of each list has the same ID and demographic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig a little deeper into finding #3. Are the ***last*** entries in both of these lists also identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'worker_id': '3876', 'gender': 'female', 'english_first_language': '1', 'age_group': '30-45', 'education': 'bachelors'}\n",
      "{'worker_id': '3876', 'gender': 'female', 'english_first_language': '1', 'age_group': '30-45', 'education': 'bachelors'}\n"
     ]
    }
   ],
   "source": [
    "print(attack_annotators[-1]) # \"-1\" tells Python to find the last item in any list\n",
    "print(aggression_annotators[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the last rows in these two lists are also identical! \n",
    "\n",
    "And in fact if you had opened the two lists in a text editor or spreadsheet program, you would find that the *aggression and attack .tsv files contain exactly the same data.* By the way, it doesn't say anywhere in the dataset documentation that these two files are identical!\n",
    "\n",
    "That brings us to our first lesson about bias: watch out for duplicate data! \n",
    "\n",
    "Consider: *What would have happened if we had just combined these three files and then analyzed the worker demographics? What mistaken conclusions might we have drawn from that?*\n",
    " \n",
    "Fortunately, now that we know that there is duplicate data we can work around it. Since two files are identical, we only need to use one of them. So from now on, we will ignore ``aggression_annotators`` entirely. \n",
    "\n",
    "Since want to remember that ``attack_annotators`` really refers to both \"attack\" and \"aggression\" annotators, we can just rename the variable we're using to store that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attack_aggro_annotators = attack_annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that looks good. Now we can continue getting our data ready for analysis--while keeping an eye out for additional duplicate data and other \"gotchas\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Understanding what the properties of your data really mean\n",
    "\n",
    "Whenever you are working with data that you didn't create, it's very useful to perform some basic sanity-checking to make sure the data actually means what you think it means. \n",
    "\n",
    "For example, take the ``worker_id`` field in our datasets. \n",
    "\n",
    "The [schema](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release#Schema_for_{attack/aggression/toxicity}_worker_demographics.tsv) says that the ``worker_id`` field contains an \"anonymized crowd-worker id\" and that this ID is meant to join the worker demographics datafiles with the annotator comments datafiles, so that if we wanted find all of the comments that worker \"85\" (from above) labelled, we could find her by looking for that ID in each row of ``toxicity_annotated_comments.tsv``.\n",
    "\n",
    "So far, so good. But since we want to combine the toxicity and attack + aggression annotator demographics data into a single dataset, we probably want to know...\n",
    "\n",
    "1. did any of the values for ``worker_id`` appear in both datasets?\n",
    "2. if so, do they correspond to the same person (or at least, a person with matching gender, age group, etc.)?\n",
    "\n",
    "FIXME WHY THIS MATTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Checking for duplicate worker IDs\n",
    "\n",
    "If we want to see if worker_id means the same thing across datasets, the first step is to see if any of the same worker_ids exist across the two datasets.\n",
    "\n",
    "To check this, let's first pull all the worker IDs out of each dataset and combine them into a single list. Then we can check that list to see if it contains any duplicate values. If it does, we know that there is at least 1 value for ``worker_id`` that appears in both datasets.\n",
    "\n",
    "**Note:** *We're assuming that there are no duplicate values for ``worker_id`` within each dataset. (There aren't, I checked). It's a pretty safe assumption though, because worker_id is intended to be a unique key that links the ``worker_demographics.tsv`` files and the ``annotated_comments.tsv`` datasets. Can you explain why it would be an issue if there were duplicate values for ``worker_id`` within an individual dataset?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull the worker ids out of the individual files\n",
    "tox_w_ids = [item['worker_id'] for item in toxicity_annotators]\n",
    "aa_w_ids = [item['worker_id'] for item in attack_aggro_annotators]\n",
    "\n",
    "\n",
    "#create a new list to hold all the ids\n",
    "all_w_ids = tox_w_ids + aa_w_ids\n",
    "\n",
    "# #combine the two worker_id lists into the new list\n",
    "# all_w_ids.extend(tox_w_ids)\n",
    "# all_w_ids.extend(aa_w_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5781\n"
     ]
    }
   ],
   "source": [
    "#how many worker ids do we have, total? \n",
    "#this number should match the total count of toxicity_annotators and attack_aggro_annotators (3591 + 2190 = 5781)\n",
    "print(len(all_w_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['85', '1617', '1394', '311', '1980']\n"
     ]
    }
   ],
   "source": [
    "#what does our new list look like? Let's print the first five values\n",
    "print(all_w_ids[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our duplicate-checker function. You pass it a list of values, and it will return \"True\" if it finds at least 1 duplicate value in that list. Can you figure out how it works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_dupes(w_ids):\n",
    "    set_of_ids = set()\n",
    "    \n",
    "    found_a_dupe = False\n",
    "    \n",
    "    for w in w_ids:\n",
    "        if w in set_of_ids:\n",
    "            found_a_dupe = True\n",
    "        else:\n",
    "            set_of_ids.add(w)\n",
    "\n",
    "    return found_a_dupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#we'll call determine_dupes to check for duplicates in the combined worker id list\n",
    "has_dupes = determine_dupes(all_w_ids)\n",
    "\n",
    "#if this prints 'True', that means we found at least one duplicate ID\n",
    "print(has_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... So it looks like there's at least 1 duplicate! Okay, we'll need to do some additional verification before we can decide what to do with that information. \n",
    "\n",
    "The next thing we'll do is check how many duplicates there are. We'll write a short script that reads through ``all_w_ids`` and every time it finds a value that appears more than once, it adds that value to a new list.\n",
    "\n",
    "Can you figure out how the script below works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3716\n",
      "1858.0\n"
     ]
    }
   ],
   "source": [
    "#create an empty list to hold any duplicate worker_id values we find\n",
    "dupes = []\n",
    "\n",
    "#look through the data, if you encounter any value more than once, add it to our 'dupes' list\n",
    "for w in all_w_ids:\n",
    "    if all_w_ids.count(w) > 1:\n",
    "        dupes.append(w)\n",
    "\n",
    "#how many values were added to 'dupes'?        \n",
    "print(len(dupes))\n",
    "\n",
    "#how many worker_ids are present twice in the dataset?\n",
    "#can you explain why we are dividing the length of \"dupes\" by 2 in order to answer that question?\n",
    "print(len(dupes)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, so it looks like 1,850 of the ``worker_ids`` in our merged dataset are duplicates! That's a lot of duplication, since our list was only 5,781 rows in the first place, including these dupes! \n",
    "\n",
    "We will definitely need to account for these duplicates before we start analyzing worker demographics. \n",
    "- If the duplicate ``worker_id``s have different demographic metadata, we will assume they are different people with the same ID. \n",
    "- If the metadata is identical, then we know we can safely remove the duplicates\n",
    "- If some metadata is identical and some is different... well, let's hope that's not the case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Check for duplicate worker demographic metadata\n",
    "Let's see if it's just the value for ``worker_id`` that is duplicated across the two datasets (meaning that the these duplicate ids correspond to different workers with different demographics), or if ``worker_id`` really corresponds to the same people across ``toxicity_annotators`` and ``attack_annotators``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we'll perform some spot checks, meaning we'll visually compare the demographic data of workers with duplicate IDs, to see if they look like the same worker, or not. Running random 'spot checks' is a really common and useful way of finding systematic issues with your data without having to check every entry. \n",
    " \n",
    "\n",
    "First, we'll extract 10 random ids from our dupe set. Using a random sample (rather than just looking at the first 10 rows, for instance) helps us be more confident that any patterns we see are really there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#handy Python library that lets you select things randomly from a list\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3573', '2614', '2972', '2010', '1816', '1138', '3200', '692', '1956', '776']\n"
     ]
    }
   ],
   "source": [
    "#convert our 'dupes' list into a set\n",
    "#bonus: can you explain why we created \"dupeset\" rather than just grabbing 10 random values from \"dupes\"?\n",
    "#hint: in Python, a 'set' is like a list that can only contain a single instance of any value\n",
    "dupeset = set(dupes)\n",
    "\n",
    "#store our random sample of dupes in its own list\n",
    "dupe_sample = random.sample(dupeset, 10)\n",
    "\n",
    "#print to confirm everything looks how we expect it to...\n",
    "print(dupe_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use this ``dupe_sample`` list that we created to pull the corresponding worker demographics from each of our two datasets, using the function below. See if you can figure out how the function works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'worker_id': '3573', 'gender': 'male', 'english_first_language': '1', 'age_group': '18-30', 'education': 'professional'}\n",
      "{'worker_id': '3573', 'gender': 'female', 'english_first_language': '0', 'age_group': '18-30', 'education': 'hs'}\n",
      "{'worker_id': '2614', 'gender': 'female', 'english_first_language': '1', 'age_group': '18-30', 'education': 'professional'}\n",
      "{'worker_id': '2614', 'gender': 'female', 'english_first_language': '0', 'age_group': '45-60', 'education': 'masters'}\n",
      "{'worker_id': '2972', 'gender': 'male', 'english_first_language': '0', 'age_group': '18-30', 'education': 'hs'}\n",
      "{'worker_id': '2972', 'gender': 'female', 'english_first_language': '0', 'age_group': '18-30', 'education': 'hs'}\n",
      "{'worker_id': '2010', 'gender': 'male', 'english_first_language': '0', 'age_group': '18-30', 'education': 'bachelors'}\n",
      "{'worker_id': '2010', 'gender': 'male', 'english_first_language': '0', 'age_group': '30-45', 'education': 'professional'}\n",
      "{'worker_id': '1816', 'gender': 'male', 'english_first_language': '1', 'age_group': '30-45', 'education': 'bachelors'}\n",
      "{'worker_id': '1816', 'gender': 'male', 'english_first_language': '0', 'age_group': '18-30', 'education': 'bachelors'}\n",
      "{'worker_id': '1138', 'gender': 'female', 'english_first_language': '0', 'age_group': '30-45', 'education': 'bachelors'}\n",
      "{'worker_id': '1138', 'gender': 'male', 'english_first_language': '0', 'age_group': '30-45', 'education': 'bachelors'}\n",
      "{'worker_id': '3200', 'gender': 'female', 'english_first_language': '1', 'age_group': '18-30', 'education': 'bachelors'}\n",
      "{'worker_id': '3200', 'gender': 'male', 'english_first_language': '1', 'age_group': '30-45', 'education': 'professional'}\n",
      "{'worker_id': '692', 'gender': 'female', 'english_first_language': '0', 'age_group': '30-45', 'education': 'doctorate'}\n",
      "{'worker_id': '692', 'gender': 'male', 'english_first_language': '1', 'age_group': '18-30', 'education': 'masters'}\n",
      "{'worker_id': '1956', 'gender': 'male', 'english_first_language': '0', 'age_group': '18-30', 'education': 'hs'}\n",
      "{'worker_id': '1956', 'gender': 'female', 'english_first_language': '0', 'age_group': '18-30', 'education': 'masters'}\n",
      "{'worker_id': '776', 'gender': 'male', 'english_first_language': '0', 'age_group': '30-45', 'education': 'professional'}\n",
      "{'worker_id': '776', 'gender': 'female', 'english_first_language': '0', 'age_group': '18-30', 'education': 'hs'}\n"
     ]
    }
   ],
   "source": [
    "def worker_id_lookup(annotator_list, dupe_id):\n",
    "    \"\"\"\n",
    "    Accepts: a list of dictionaries \n",
    "        & a list of known duplicate values for\n",
    "        the key 'worker_id' in those dictionaries\n",
    "    \n",
    "    If a duplicate value for worker_id is found, \n",
    "        print the complete dictionary of demographic data for that worker\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for a in annotator_list:\n",
    "        if a['worker_id'] == dupe_id:\n",
    "            print(a)\n",
    "            \n",
    "#loop through the duplicate sample list and call our worker_id_lookup function\n",
    "#to check each dataset for corresponding worker demographic data\n",
    "for d in dupe_sample:\n",
    "    worker_id_lookup(toxicity_annotators, d)\n",
    "    worker_id_lookup(attack_aggro_annotators, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! It looks like few if any of these ``worker_id`` values correspond to the same demographic data info across the two datasets. \n",
    "\n",
    "It's possible that some of the same people labelled both datasets and were assigned different IDs each time. But there's no way for us to determine that with the data we have. \n",
    "\n",
    "So for now we will assume that these datasets were labelled by two entirely different sets of crowdworkers. Which means we can combine these two datasets into one, with no duplication to mess up our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Final preparation of the dataset\n",
    "\n",
    "Before we start our analysis of worker demographics, let's do two more things:\n",
    "\n",
    "1. Since we know now that worker ID isn't unique, let's give each worker in our new, combined dataset a **truly** unique ID.\n",
    "2. While we're at it, let's also add a new field to each worker's demographic dictionary that lists which dataset the worker worked on (toxicity or attack/aggressive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dataset_id(list_of_dicts, dataset_ref):\n",
    "    \"\"\"\n",
    "    Accepts: a list of dictionaries & a strig value for \"dataset\"\n",
    "    Returns: that list of dictionaries, with the new \"dataset\" key and the specified value\n",
    "    \"\"\"\n",
    "        \n",
    "    for w in list_of_dicts:\n",
    "        w.update({\"dataset\" : dataset_ref})\n",
    "        \n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#update our worker demographic datasets with the new key and value\n",
    "toxicity_annotators = add_dataset_id(toxicity_annotators, \"toxicity\")\n",
    "attack_aggro_annotators = add_dataset_id(attack_aggro_annotators, \"attack and aggression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'worker_id': '85', 'gender': 'female', 'english_first_language': '0', 'age_group': '18-30', 'education': 'bachelors', 'dataset': 'toxicity'}\n",
      "{'worker_id': '833', 'gender': 'female', 'english_first_language': '0', 'age_group': '45-60', 'education': 'bachelors', 'dataset': 'attack and aggression'}\n"
     ]
    }
   ],
   "source": [
    "#did it work?\n",
    "print(toxicity_annotators[0])\n",
    "print(attack_aggro_annotators[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we'll use the function below to \n",
    "\n",
    "1. combine the two datasets into one\n",
    "2. assign a truly unique id to each worker \n",
    "\n",
    "It doesn't matter what this ID is, as long as its unique within the dataset. We'll use sequential IDs, starting at \"1\" and going up from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finalize_dataset(list1, list2):\n",
    "    \n",
    "    #combine the two lists into a new one\n",
    "    combined_list = list1 + list2\n",
    "    \n",
    "    #initialize our counter variable\n",
    "    new_id = 1\n",
    "    \n",
    "    for w in combined_list:\n",
    "        #add the new sequential id field, and populate with the current value of 'counter'\n",
    "        w.update({\"unique_id\" : str(new_id)})\n",
    "        \n",
    "        #increment the counter variable by 1, so that the next ID will be one number higher\n",
    "        new_id = new_id + 1\n",
    "        \n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5781\n",
      "{'worker_id': '85', 'gender': 'female', 'english_first_language': '0', 'age_group': '18-30', 'education': 'bachelors', 'dataset': 'toxicity', 'unique_id': '1'}\n"
     ]
    }
   ],
   "source": [
    "#create our new combined list with sequential IDs\n",
    "all_annotators = finalize_dataset(toxicity_annotators, attack_aggro_annotators)\n",
    "\n",
    "#print the list length and a sample, for sanity-checking purposes\n",
    "print(len(all_annotators))\n",
    "print(all_annotators[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! Now we can FINALLY start to analyze this dataset, and find out more about the workers who labelled the Wikipedia Talk Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Analyzing worker demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our worker demographic data de-duplicated, combined and uniquely identified, we can start using it to ask and answer research questions! \n",
    "\n",
    "You are welcome to do this next steps in Python, here in this notebook. But if you aren't super comfortable with Python, you can run the cell below to export this dataset to a .csv file called ``a2_all_annotator_demographics.csv`` which you can open in Google Sheets and do the analysis there.\n",
    "\n",
    "Note: if you choose Google Sheets, please title your Google Sheet \"A2 worker demographics\", share it with your instructor and TA (\"can view\" or \"can edit\") and paste a link to it in a Markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('a2_all_annotator_demographics.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    #write a header row\n",
    "    writer.writerow(('unique_id',\n",
    "                     'worker_id', \n",
    "                     'gender', \n",
    "                     'english_first_language', \n",
    "                     'age_group',\n",
    "                     'education',\n",
    "                     'dataset',))\n",
    "    #loop through our dataset and write it to the file, row by row\n",
    "    for a in all_annotators:\n",
    "        writer.writerow((a['unique_id'], a['worker_id'], a['gender'], a['english_first_language'], a['age_group'], a['education'], a['dataset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Questions 1-4: compute descriptive statistics\n",
    "\n",
    "Please create tables or bar charts that answer the following questions:\n",
    "\n",
    "1. What is the gender distribution of these workers?\n",
    "2. What is the distribution of the workers by first language?\n",
    "3. What is the distribution of these workers by age group?\n",
    "4. what is the distribution of these workers by education level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Questions 5-8\n",
    "\n",
    "Each of the next four questions requires a different kinds of answer.\n",
    "\n",
    "Questions 5 and 6 have a \"correct\" answer, but we'll still give you credit for the \"incorrect\" answer as long as you document your process well (in Markdown, in this notebook). These questions are intended to get you thinking about how even small errors in data processing can create bias in your data.\n",
    "\n",
    "Question 6 requires you to re-run some (but not all) of the data processing steps you performed above. Do this by copying the code into NEW cells below this line, and make sure to document each step using Markdown cells or inline (#) comments.\n",
    "\n",
    "5. (analyze all_annotators in Sheets or Jupyter): Do any of these fields have missing data? If so, which fields, and what % of the rows contain missing values?\n",
    "6. (re-build the all_annotators dataset in Jupyter without removing duplicates first) How would these summary statistics change if we hadn't de-duplicated the data?\n",
    "\n",
    "Questions 7 and 8 don't have one \"correct\" answer. These questions are intended to get you thinking about how using this dataset to train a machine learning model might lead to bias in the way that model performs when used as intended.   You'll need to provide a written answer to each of these questions in 3-4 full sentences. Consider what you know about the workers themselves, the task they were given, and about the purpose of the Perspective API. \n",
    "\n",
    "7. (perform some internet research using Google) In general, how do the demographics of these workers compare to those of English-speaking internet users overall? \n",
    "\n",
    "8. (think critically about the consequences bias in data) given what you now know about the Wikipedia Talk Corpus, what issues might arise if it was used to train a machine learning-driven \"hostile speech detector\" that could be used on any website or social media platform?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Challenge questions\n",
    "\n",
    "Below is a list of additional questions that you should be able to answer, based on what you've done today. You can answer ANY TWO of these questions. Some require additional coding, others don't. \n",
    "\n",
    "For the \"code\" questions, you will need to load ``toxicity_annotated_comments.tsv`` into Python and join that dataset with ``all_annotators`` on ``worker_id``.\n",
    "\n",
    "For the \"no code\" questions, you will need to load ``toxicity_annotated_comments.tsv`` AND THE OTHERS into Google Sheets. \n",
    "\n",
    "- (code) What % of workers the toxicity dataset do we have demographic data for?\n",
    "- (code) What % of the comments in the toxicity dataset were labelled by male vs. female-identified workers?\n",
    "- (code) What % of the comments in the toxicity dataset were labelled by people for whom English is not their first language?\n",
    "- (no code) Read through the instructions and labelling options given to the crowdworkers. How might the instructions given to the workers FIXME influenced how they labelled the data? Could the way they understood the instructions have led them to bias FIXME?\n",
    "- (no code) Read through at least 20 comments from ``toxicity_annotated_comments.tsv``. (hint: load the file into Google Sheets) How confident do you feel that you could accurately identify \"toxic\" speech in comments like these? What, if anything, about these comments makes them easy or hard to label for \"toxicity\" (assume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. ***Do we have demographic data for all workers?*** The schema says that this demographic data \"was obtained by an **optional** demographic survey administered after the labelling task\". If only a small number of the workers who annotated the data filled out the survey, then we might not end up with a very representative picture of who these workers were.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to know whether the person identified as \n",
    "\n",
    "That makes sense, but it doesn't tell us everything we need to know before we start our analysis. \n",
    "\n",
    "1. ***Do identical worker IDs in ``toxicity_annotators`` and ``attack_aggro_annotators`` refer to the same worker?*** If we are going to create a clean list that contains demographic information about all of the annotators, we need to know if ``worker_id`` in one file corresponds to the same ``worker_id`` in another file. In other words, if the same crowdworker participated in labelling the \"toxicity\" dataset and the \"aggression\" dataset, are they identified by the same ID? \n",
    "\n",
    "FIXME FIXME ALSO do we have demographics about all workers?\n",
    "\n",
    "However, if we want to analyze worker demographics, we still need to know: ***does ``worker_id`` mean the same thing ACROSS different datasets?*** \n",
    "\n",
    "In other words, if we see ``worker_id`` \"85\" appears in both .tsv files, does that mean that worker 1234 participated in both toxic and attack/aggression labelling campaigns?\n",
    "\n",
    "If so, we would need to de-duplicate our combined worker list in order to pull accurate demographic data about the workers.\n",
    "\n",
    "To check this, let's combine our two ``worker_id`` lists and then run ``determine_dupes`` to see if there's any overlap.\n",
    "\n",
    "\n",
    "If ``worker_id`` is unique...\n",
    "- it will be easy for us to see whether any of the same workers annotated both the \"toxicity\" dataset and the \"aggression and personal attacks\" datasets\n",
    "- we can pool the two worker datasets and look for overall demographic trends among the workers\n",
    "- we can examine patterns in worker behavior, such as whether male-identified workers tended to identify more \"bad\" (toxic, aggressive, attacky) behavior than female-identified workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
